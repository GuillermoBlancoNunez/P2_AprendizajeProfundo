{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3adf3e6f",
   "metadata": {},
   "source": [
    "Guillermo Blanco Núñez <br>\n",
    "Pablo Díaz Blanco<br>\n",
    "ap-2526-p3-ap-11-01\n",
    "\n",
    "\n",
    "# Práctica 3: RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f1ca3",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "\n",
    "Los objetivos de la práctica son:\n",
    "1. Diseñar y entrenar una red neuronal residual lineal para un problema de clasificación binaria.\n",
    "2. Implementar un método de fine-tuning eficiente que permita adaptar el modelo a una variante del\n",
    "problema sin volver a entrenar todos los parámetros.\n",
    "3. Analizar las ventajas y limitaciones del método"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44633b82",
   "metadata": {},
   "source": [
    "## Preparación del dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bd9987",
   "metadata": {},
   "source": [
    "Importa todas las librerías necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "041a98d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68388192",
   "metadata": {},
   "source": [
    "Función para cargar archivos .pkl (pickle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1af614a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pkl(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2242283f",
   "metadata": {},
   "source": [
    "Importa el split del dataset Higgs, descargado del aula virtual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e859d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../Higgs_DS/\"\n",
    "\n",
    "train_file = os.path.join(DATA_PATH, \"higgs_train.pkl\")\n",
    "test_file  = os.path.join(DATA_PATH, \"higgs_test.pkl\")\n",
    "extra_file = os.path.join(DATA_PATH, \"higgs_extra.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "train_data = load_pkl(train_file)\n",
    "test_data  = load_pkl(test_file)\n",
    "extra_data = load_pkl(extra_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146215b",
   "metadata": {},
   "source": [
    "Comprueba que el conjunto de datos esté correctamente importado al mostrar por pantalla esta información del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a106372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2000000, 29)\n",
      "Test shape: (500000, 29)\n",
      "Extra shape: (2000000, 29)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape:\", train_data.shape)\n",
    "print(\"Test shape:\", test_data.shape)\n",
    "print(\"Extra shape:\", extra_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1850739",
   "metadata": {},
   "source": [
    "Convertir a arrays de numpy y separar los datos de las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c05f95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_np = np.asarray(train_data)\n",
    "test_np  = np.asarray(test_data)\n",
    "\n",
    "X = train_np[:, :-1].astype(np.float32)\n",
    "y = train_np[:, -1].astype(np.float32)\n",
    "\n",
    "X_test = test_np[:, :-1].astype(np.float32)\n",
    "y_test = test_np[:, -1].astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd53fa",
   "metadata": {},
   "source": [
    "Creación de train y validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77131061",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3f0501",
   "metadata": {},
   "source": [
    "Calculo de la media y la desviación típica y aplicar normalización "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16a4eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train = X_train.mean(axis=0)\n",
    "std_train  = X_train.std(axis=0)\n",
    "\n",
    "std_train_safe = np.where(std_train == 0, 1.0, std_train)\n",
    "\n",
    "X_train_norm = (X_train - mean_train) / std_train_safe\n",
    "X_val_norm   = (X_val   - mean_train) / std_train_safe\n",
    "X_test_norm  = (X_test  - mean_train) / std_train_safe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de8303",
   "metadata": {},
   "source": [
    "Comprobación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c44e0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1600000, 28) (1600000,)\n",
      "Val:   (400000, 28) (400000,)\n",
      "Test:  (500000, 28) (500000,)\n",
      "\n",
      "Media primeras 5 features del TRAIN normalizado:\n",
      "[ 1.1567707e-06 -2.7320066e-05  4.5669823e-09]\n",
      "\n",
      "STD primeras 5 features del TRAIN normalizado:\n",
      "[0.9983659 0.9999674 1.0000007]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\", X_train_norm.shape, y_train.shape)\n",
    "print(\"Val:  \", X_val_norm.shape,   y_val.shape)\n",
    "print(\"Test: \", X_test_norm.shape,  y_test.shape)\n",
    "\n",
    "print(\"\\nMedia primeras 5 features del TRAIN normalizado:\")\n",
    "print(X_train_norm.mean(axis=0)[:3])\n",
    "\n",
    "print(\"\\nSTD primeras 5 features del TRAIN normalizado:\")\n",
    "print(X_train_norm.std(axis=0)[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72adfd",
   "metadata": {},
   "source": [
    "## Definición de los hiperparámetros\n",
    "- Constantes: \n",
    "    1. EPOCHS: número máximo de épocas de entrenamiento para cada configuración.\n",
    "    2. BATCH_SIZE: número de ejemplos que se procesan juntos en cada actualización de pesos.\n",
    "    3. INPUT_SIZE: número de características de entrada (columnas) del dataset Higgs\n",
    "    4. USE_EARLY_STOPPING: indica si se usa parada temprana para evitar sobreentrenar.\n",
    "    5. PATIENCE: número de épocas sin mejora permitido antes de activar el early stopping.\n",
    "\n",
    "- Cada tupla de la lista hyperparams representa un conjunto de hiperparámetros con la forma:\n",
    "    \n",
    "    (used_dim, n_blocks, opt_name, dropout, use_batchnorm)\n",
    "- Explicación de cada uno:\n",
    "    1. used_dim: tamaño del espacio interno, es decir, número de neuronas en cada capa densa de los bloques residuales. \n",
    "    2. n_blocks: número de bloques residuales encadenados. Cada bloque incluye dos capas lineales y una conexión residual.\n",
    "    3. opt_name: tipo de optimizador utilizado.\n",
    "    4. dropout: porcentaje de neuronas que se apagan aleatoriamente tras la activación (solo cuando no se usa BatchNorm).\n",
    "    5. use_batchnorm: booleano que indica si se aplica Batch Normalization en las capas densas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 512\n",
    "INPUT_SIZE = X_train_norm.shape[1]\n",
    "PATIENCE = 20\n",
    "\n",
    "hyperparams = [\n",
    "    (64,  2, \"Adam\",    0.0, False),\n",
    "    (64,  2, \"Adam\",    0.2, False),\n",
    "    (128, 2, \"Adam\",    0.0, False),\n",
    "    (128, 2, \"Adam\",    0.3, False),\n",
    "\n",
    "    (64,  2, \"Adam\",    0.0, True),\n",
    "    (128, 3, \"Adam\",    0.0, True),\n",
    "\n",
    "    (64,  2, \"RMSprop\", 0.0, False),\n",
    "    (64,  3, \"SGD\",     0.0, False),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be76e2",
   "metadata": {},
   "source": [
    "## Construcción de una red residual\n",
    "- build_residual_MLP: construye y compila una red residual densa con bloques residuales para el problema de clasificación binaria del dataset Higgs. Recibe como entrada el tamaño del vector de características y los hiperparámetros del modelo. Primero crea una capa de entrada y otra densa inicial. Luego construye n_blocks capas residuales, donde cada una tiene dos capas lineales. El resultado de esto se suma a la entrada original aplicando ReLU al final. Por último, añade una capa Dense con activación sigmoide para clasifiación binaria. Selecciona el optimizador y crea el callback de early stopping. Los parámetros son:\n",
    "    1. input_size: número de características de entrada.\n",
    "    2. used_dim: número de neuronas en las capas densas internas de la red\n",
    "    3. n_blocks: número de bloques residuales encadenados. \n",
    "    4. opt_name: nombre del optimizador a utilizar para el entrenamiento.\n",
    "    5. dropout: tasa de dropout aplicada tras la activación cuando no se usa Batch Normalization.\n",
    "    6. use_batchnorm: booleano que indica si se aplica Batch Normalization en las capas densas.\n",
    "    7. use_early_stopping: indica si se debe crear y devolver un callback de parada temprana (EarlyStopping) junto con el modelo.\n",
    "    8. patience: número de épocas sin mejora en la métrica de validación permitidas antes de activar el early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_residual_MLP(input_size, used_dim, n_blocks, opt_name, dropout, use_batchnorm):\n",
    "\n",
    "    if use_batchnorm and dropout > 0.0:\n",
    "        print(\"[AVISO] use_batchnorm=True y dropout>0. Ignorando dropout para no mezclar BN y Dropout.\")\n",
    "        dropout = 0.0\n",
    "\n",
    "    inputs = layers.Input(shape=(input_size,))\n",
    "\n",
    "    x = layers.Dense(used_dim)(inputs)\n",
    "\n",
    "    if use_batchnorm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "    else:\n",
    "        x = layers.ReLU()(x)\n",
    "        if dropout > 0.0:\n",
    "            x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    for i in range(n_blocks):\n",
    "        residual = x  \n",
    "\n",
    "        z = layers.Dense(used_dim)(x)\n",
    "        if use_batchnorm:\n",
    "            z = layers.BatchNormalization()(z)\n",
    "            z = layers.ReLU()(z)\n",
    "        else:\n",
    "            z = layers.ReLU()(z)\n",
    "            if dropout > 0.0:\n",
    "                z = layers.Dropout(dropout)(z)\n",
    "\n",
    "        z = layers.Dense(used_dim)(z)\n",
    "        if use_batchnorm:\n",
    "            z = layers.BatchNormalization()(z)\n",
    "\n",
    "        x = layers.Add()([residual, z])\n",
    "        x = layers.ReLU()(x)\n",
    "\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    if opt_name == \"Adam\":\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    elif opt_name == \"SGD\":\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=0.03, momentum=0.9)\n",
    "    elif opt_name == \"RMSprop\":\n",
    "        opt = tf.keras.optimizers.RMSprop(learning_rate=1e-3, rho=0.9)\n",
    "    else:\n",
    "        raise ValueError(f\"Optimiser '{opt_name}' not supported\")\n",
    "\n",
    "    model.compile(optimizer=opt)\n",
    "\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcfa5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets(X_train, y_train, X_val, y_val, batch_size):\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    train_ds = train_ds.shuffle(buffer_size=len(X_train)).batch(batch_size)\n",
    "\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "    val_ds = val_ds.batch(batch_size)\n",
    "\n",
    "    return train_ds, val_ds\n",
    "\n",
    "\n",
    "def train_one_model(\n",
    "    model,\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    patience=PATIENCE\n",
    "):\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "    train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "    val_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "    train_ds, val_ds = make_datasets(X_train, y_train, X_val, y_val, batch_size)\n",
    "\n",
    "    history = []\n",
    "    best_val_acc = 0.0\n",
    "    best_weights = model.get_weights()\n",
    "    best_epoch = 0\n",
    "    epochs_without_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        train_loss_sum = 0.0\n",
    "        train_steps = 0\n",
    "\n",
    "        for step, (x_batch, y_batch) in enumerate(train_ds):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x_batch, training=True)\n",
    "                loss_value = loss_fn(y_batch, logits)\n",
    "                if model.losses:\n",
    "                    loss_value += tf.add_n(model.losses)\n",
    "\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "            train_acc_metric.update_state(y_batch, logits)\n",
    "\n",
    "            train_loss_sum += float(loss_value.numpy())\n",
    "            train_steps += 1\n",
    "\n",
    "        train_loss = train_loss_sum / train_steps\n",
    "        train_acc = float(train_acc_metric.result().numpy())\n",
    "        train_acc_metric.reset_states()\n",
    "\n",
    "        val_loss_sum = 0.0\n",
    "        val_steps = 0\n",
    "\n",
    "        for x_batch_val, y_batch_val in val_ds:\n",
    "            logits_val = model(x_batch_val, training=False)\n",
    "            loss_val = loss_fn(y_batch_val, logits_val)\n",
    "            if model.losses:\n",
    "                loss_val += tf.add_n(model.losses)\n",
    "\n",
    "            val_acc_metric.update_state(y_batch_val, logits_val)\n",
    "\n",
    "            val_loss_sum += float(loss_val.numpy())\n",
    "            val_steps += 1\n",
    "\n",
    "        val_loss = val_loss_sum / val_steps\n",
    "        val_acc = float(val_acc_metric.result().numpy())\n",
    "        val_acc_metric.reset_states()\n",
    "\n",
    "        print(\n",
    "            f\"  train_loss={train_loss:.4f}  \"\n",
    "            f\"train_acc={train_acc:.4f}  \"\n",
    "            f\"val_loss={val_loss:.4f}  \"\n",
    "            f\"val_acc={val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        history.append(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_acc\": val_acc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if val_acc > best_val_acc + 1e-4:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch + 1\n",
    "            best_weights = model.get_weights()\n",
    "            epochs_without_improve = 0\n",
    "        else:\n",
    "            epochs_without_improve += 1\n",
    "            if patience is not None and epochs_without_improve >= patience:\n",
    "                print(\n",
    "                    f\"Early stopping: sin mejora en val_acc durante {patience} épocas.\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "    return history, best_weights, best_epoch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AP-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
